\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pgfplots}
a2174d28-2dad-11e5-90cf-3c15c2c7a012\usepackage{hyperref}
\usepackage{breakurl}
a2174d28-2dad-11e5-90cf-3c15c2c7a012\usepackage{indentfirst}
\usepackage{authblk}
\usepackage{todonotes}
\usepackage[doublespacing]{setspace}

\usepackage[backend=biber]{biblatex}
\addbibresource{ZTree.bib}
a2174d28-2dad-11e5-90cf-3c15c2c7a012
\title{The Z-Table: A Novel Range-Queryable Distributed Data Structure for Use in P2P Systems}
a2174d28-2dad-11e5-90cf-3c15c2c7a012

\begin{document}

a2174d28-2dad-11e5-90cf-3c15c2c7a012\maketitle

\begin{abstract}
We present the design and implementation of a novel data structure (the 'Z-Table'). We aim to solve the issue of window/range-based queries in peer to peer architectures. Traditional models, for example,  distributed hash tables (DHT), are hostile towards window queries because their hashing operations are designed to uniformly distribute stored data across a defined key space; the hashing operations used to achieve this pseudo-random distribution inherently erases all characteristics of the target data that could be used to define locality. We solve this problem of erasure by defining a scheme in which higher-order data is mapped to a first-dimensional key space, while preserving locality. The resulting keys pace is very definitely not uniformly distributed, so we define a distributed consensus scheme in which participants in our Z-Tables agree to target highly populated regions of the key space. This consensus scheme also provides some protection from Sybil attacks. Finally, we define storage, lookup, and deletion operations that utilize balanced search trees to efficiently perform necessary network functions; the preservation of locality allows us to greatly optimize these operations through the use of balanced trees. A peer to peer communication system acts as the underlying network for participants, providing all of the traditional benefits of a P2P architecture (fault tolerance, scalability, and truly independent operation).
\end{abstract}

\section{Notation}

a2174d28-2dad-11e5-90cf-3c15c2c7a012Tuples (concatenation): $[a,b,c,...]$

Application of a function $f$ to an input $x$: $y=f(x)$

a2174d28-2dad-11e5-90cf-3c15c2c7a012Application of a hash function $H$ with $k$ output bits: $H_{k}(x) = \{0,1\}^k$

Keys of user $j$: $ PUBK_j; PRIVK_j $

Note: $PUBK(PRIVK(x)) = x$ and $PRIVK(PUBK(y)) = y$~\todo{citation}
a2174d28-2dad-11e5-90cf-3c15c2c7a012
Signed content $x$ from user $j$: $SIG_j(x) = \left[ PRIVK_j( H(x) ), x \right]$
a2174d28-2dad-11e5-90cf-3c15c2c7a012
a2174d28-2dad-11e5-90cf-3c15c2c7a012\section{Introduction}
\par Distributed hash tables are currently one of the hottest topics in the cryptography space~\cite{Stoica:2001dj,Rowstron:2001ea,Ratnasamy:2001wn}. They are used in many large scale open sources projects~\cite{Freitas:2013tb,Xu:2010vs,Perfitt:2010fh} as their underlying infrastructure. First, in order to better understand the motivation behind this project, a brief explanation of the workings of a distributed has table (DHT) and its constituent protocols is needed.

a2174d28-2dad-11e5-90cf-3c15c2c7a012\par Simply put, DHTs were created to provide hash table-like functionality on an Internet-scale level~\cite{Ratnasamy:2001wn}. They expose basic PUT and GET requests (operating on (key,value) pairs) to clients, allowing them to store and retrieve information from the DHT. The difference between a hash table and a DHT is the scope of the mechanism: DHTs are able to operate in an entirely decentralized setting, in which the responsibility for the mapping of keys to values is distributed pseudo-uniformly among all participants in a P2P network. Notable networks that utilize DHTs include the Coral Content Distribution Network~\cite{Freedman:2004vb}, the Storm Botnet~\cite{Holz:2008uk}, and the BitTorrent file sharing protocol~\cite{Cohen:y1_8mBnw}.

\par Keys used in traditional DHTs are calculated through the use of a hash function. Given that the digest (output) of a hash function is some number of bits, each of which has the same probability of being set to $'1'$ or $'0'$, we can confidently conclude that the outputs will be randomly distributed along the hash function's keyspace~\todo{citation}. This keyspace is the set of all possible outputs; for a hash function with $k$ output bits, this is the set $\{0,1\}^k$. This is useful to users in a DHT because, assuming that a given hash function is 'sound,' in that its outputs are actually evenly distributed, they are able to mathematically verify the even partitioning of the keyspace. The participants can thus trust that both the load will be balanced fairly among all nodes. No one node will have complete control, and no group of nodes will be unfairly burdened due to their being 'hotspots' in the keyspace.~\todo{citations?}
a2174d28-2dad-11e5-90cf-3c15c2c7a012
\par One important continuity between all of these examples is the use of a DHT in an 'exact-match' lookup scenario; a normal hash table has this same limitation (a lack of the ability to perform range-queries), however, locally, the move to a more friendly structure is trivial. This is not so for a DHT, due to the nature of
\printbibliography
a2174d28-2dad-11e5-90cf-3c15c2c7a012\end{document}
